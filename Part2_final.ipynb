{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(file):\n",
    "    \"\"\"\n",
    "    :param file: input data file \n",
    "    :return: Dict in the form of {data:[[tweets],[sentiments]], x_set:{tweets}, y_set:{sentiments}}\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    dictionary = {}\n",
    "    allKeys = []\n",
    "    allVals = []\n",
    "    \n",
    "    with open(file,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        index=0\n",
    "        for i in range (len(lines)-2000):\n",
    "\n",
    "            if lines[i] == '\\n':\n",
    "                data.append(lines[index:i]) #append everything until it encounters a \\n\n",
    "                index = i+1\n",
    "            lines[i] = lines[i].replace('\\n','')# replace the \\n at the end of each sentiments\n",
    "            lines[i] = lines[i].split(' ') #split the line into their respective parts  \n",
    "\n",
    "        #convert to keys and values (dict)\n",
    "        for i in range(len(data)):\n",
    "            data_values = data[i]\n",
    "            for j in range(len(data_values)):\n",
    "                if len(data_values[j]) > 2:\n",
    "                    for k in range(1, len(data_values[j]) - 1):\n",
    "                        data_values[j][0] += \" \"\n",
    "                        data_values[j][0] += data_values[j][k]\n",
    "\n",
    "            key = [word[0] for word in data_values] #tweet\n",
    "            val = [word[-1] for word in data_values] #sentiment\n",
    "\n",
    "            data[i] = [key,val]\n",
    "\n",
    "        #data[i][0] gives the tweets\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i][0])):\n",
    "                allVals.append(data[i][1][j]) #appending the sentiments corresponding to the tweets\n",
    "             \n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i][0])):\n",
    "                #print(data[i][0][j]) #give each word\n",
    "                allKeys.append(data[i][0][j])\n",
    "        setKeys = set(allKeys)\n",
    "        setVals = set(allVals)   \n",
    "\n",
    "    return dict(data=data,x_set=setKeys,y_set=setVals)\n",
    "\n",
    "\n",
    "#obtain_data('sg_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-negative</th>\n",
       "      <th>B-positive</th>\n",
       "      <th>O</th>\n",
       "      <th>I-positive</th>\n",
       "      <th>I-negative</th>\n",
       "      <th>I-neutral</th>\n",
       "      <th>B-neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>denver</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@annebethasha</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#superpredators</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#TheFlash</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 B-negative  B-positive         O  I-positive  I-negative  \\\n",
       "denver                  0.0    0.000000  0.000000         0.0         0.0   \n",
       "@annebethasha           0.0    0.000000  0.000004         0.0         0.0   \n",
       "#superpredators         0.0    0.000000  0.000004         0.0         0.0   \n",
       "refs                    0.0    0.000000  0.000017         0.0         0.0   \n",
       "#TheFlash               0.0    0.000154  0.000004         0.0         0.0   \n",
       "\n",
       "                 I-neutral  B-neutral  \n",
       "denver                 0.0   0.000056  \n",
       "@annebethasha          0.0   0.000000  \n",
       "#superpredators        0.0   0.000000  \n",
       "refs                   0.0   0.000000  \n",
       "#TheFlash              0.0   0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#e(x|y) = Count(y -> x)/Count(y)\n",
    "#Count(y->x) means number of times you see x generated from y\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_emission_count(parsed_data):\n",
    "    \"\"\"\n",
    "    :param parsed_data: input preprocessed dataset\n",
    "    :return: emissions dataframe and count(y)\n",
    "    ,where emissions dataframe is the count of the sentiments tagged to the tweet whilecount(y) is the number of times the sentiment appears\n",
    "    \"\"\"\n",
    "    data = parsed_data['data']\n",
    "    x_set = parsed_data['x_set']\n",
    "    y_set = parsed_data['y_set']\n",
    "    #create a new datafram of zeros with keys (ie.tweets) as the index and sentiments as the columns\n",
    "    count_emissions_df = pd.DataFrame(np.zeros((len(x_set),len(y_set))),index=x_set,columns=y_set)\n",
    "    count_y = pd.Series(np.zeros(len(y_set)),index=y_set) #create a series object of zeros with index as the sentiments => to store the number times the sentiments appear\n",
    "    #print(count_y)\n",
    "    #print(count_emissions_df) #datafram structure: where its tweets against columns of sentiments\n",
    "    \n",
    "    for word in data:\n",
    "        #print(word) #format of data => [[keys],[values]]\n",
    "        #keys are the tweets, values are the sentiments\n",
    "        tweets_data,sentiments_data = word\n",
    "        \n",
    "        for i in range(len(tweets_data)):\n",
    "            tweet,sentiment = tweets_data[i],sentiments_data[i] #associate the tweet with its sentiment\n",
    "            #print(tweet,sentiment)\n",
    "            #print(sentiment)\n",
    "            #+1 to the row,col, given the tweet, sentiment freq +1\n",
    "            count_emissions_df.loc[tweet,sentiment] += 1 #.loc[] access a grp of rows and columns by labels\n",
    "            #count_emissions_df is for Count(y->x) [counting the number of times a sentiment wrt to the tweet]\n",
    "            count_y[sentiment] += 1 #incrementing the number of time the respective sentiment appear\n",
    "    return count_emissions_df,count_y\n",
    "    \n",
    "def get_emission_params(parsed_data):\n",
    "    count_emissions_df,count_y = calculate_emission_count(parsed_data)\n",
    "    return count_emissions_df/count_y #e(x|y), where x is the tweet, and y is the sentiment\n",
    "\n",
    "\n",
    "em_df = get_emission_params(obtain_data('sg_train'))\n",
    "#get_emission_counts(obtain_data('sg_train'))\n",
    "em_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B-negative', 'B-positive', 'O', 'I-positive', 'I-negative',\n",
      "       'I-neutral', 'B-neutral'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-negative</th>\n",
       "      <th>B-positive</th>\n",
       "      <th>O</th>\n",
       "      <th>I-positive</th>\n",
       "      <th>I-negative</th>\n",
       "      <th>I-neutral</th>\n",
       "      <th>B-neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>goreng</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jäger</th>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mane</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tigre</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#UNK#</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        B-negative  B-positive         O  I-positive  I-negative  I-neutral  \\\n",
       "goreng    0.000000    0.000154  0.000000    0.000235         0.0    0.00000   \n",
       "jäger     0.000292    0.000000  0.000000    0.000000         0.0    0.00000   \n",
       "Mane      0.000000    0.000000  0.000000    0.000235         0.0    0.00000   \n",
       "Tigre     0.000000    0.000000  0.000000    0.000000         0.0    0.00006   \n",
       "#UNK#     0.000000    0.000000  0.000004    0.000000         0.0    0.00000   \n",
       "\n",
       "        B-neutral  \n",
       "goreng        0.0  \n",
       "jäger         0.0  \n",
       "Mane          0.0  \n",
       "Tigre         0.0  \n",
       "#UNK#         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_new_emission_counts(parsed_data,k):\n",
    "    \"\"\"\n",
    "    :param parsed_data: input preprocessed dataset\n",
    "    :param k:  number of occurrences\n",
    "    :return: new dataframe with failed tweets replaced with #UNK# and count(y)\n",
    "    \"\"\"\n",
    "    \n",
    "    count_emissions_df,count_y = calculate_emission_count(parsed_data)\n",
    "    #.sum(axis = 1(sum the column), axis = 0 (sum the index))\n",
    "    count_tweet_appearance = count_emissions_df.sum(axis=1)\n",
    "    #print(count_tweet_appearance) #counting the number of times each tweet appears by summing everything across the columns\n",
    "    '''Output of count_tweet appearance eg. \n",
    "        seems                      15.0\n",
    "        https://t.co/h6Ie4IBJ08     1.0\n",
    "        #AnnaVonHausswolff          2.0\n",
    "        Bowery                      2.0\n",
    "        refuge                      2.0\n",
    "        @chuckielufc                1.0\n",
    "        https://t.co/7xSNeWemp1     1.0\n",
    "        @chris_steller              3.0\n",
    "        unexpected                  3.0\n",
    "        #usantdp                    1.0\n",
    "        Ones                        2.0\n",
    "        1979                        4.0\n",
    "        @joceltsh                   1.0\n",
    "        @TomBoxingAsylum            2.0\n",
    "        @thistletat13               2.0\n",
    "        @eibeibb                    2.0\n",
    "        @TalatHussain12             1.0\n",
    "        Ilkeston                    2.0\n",
    "        @ricosua                    1.0\n",
    "        Belarus                     2.0\n",
    "        charms                      2.0\n",
    "        @EvermorSolution            2.0\n",
    "        https://t.co/WrcuWKQ0Xg     2.0\n",
    "        FIRED                       2.0'''\n",
    "    \n",
    "    failed_tweets = count_tweet_appearance[count_tweet_appearance<k]\n",
    "    #print(failed_tweets)\n",
    "    '''eg output if k<3 (ie. tweets with occurence less than 3 times) is:\n",
    "        @Nandos                    1.0\n",
    "        @rcmpgrcpolice             1.0\n",
    "        #yas                       1.0\n",
    "        @just                      2.0\n",
    "        ford                       1.0\n",
    "        attracted                  2.0\n",
    "        @Unitetheunion             1.0\n",
    "        .....        '''\n",
    "    \n",
    "    #replace the tweets that occur less than 1.0 with \"#UNK#\"\n",
    "    #print(failed_tweets.index) #gives all the tweets that <1.0\n",
    "    \n",
    "    replace_tweets = count_emissions_df.loc[failed_tweets.index].sum(axis=0)\n",
    "    replace_tweets.name  = '#UNK#'\n",
    "    \n",
    "    new_df = count_emissions_df.append(replace_tweets)\n",
    "    new_df = new_df.drop(failed_tweets.index,axis=0) #drop all failed_tweets words\n",
    "    headers = new_df.dtypes.index\n",
    "    print(headers)\n",
    "    #print(headers[2]) #gives sentiment O\n",
    "    new_df.at['#UNK#',headers[2]] = 1.0\n",
    "    #print(new_df) #without failed_tweets words inside, has #UNK# row inside at the bottom\n",
    "    \n",
    "    return new_df, count_y\n",
    "\n",
    "def get_new_emission_params(parsed_data,k):\n",
    "    \"\"\"\n",
    "    :param parsed_data: input preprocessed dataset\n",
    "    :param k: number of occurrences\n",
    "    :return: new emission params \n",
    "    \"\"\"\n",
    "    count_emissions_df,count_y = calculate_new_emission_counts(parsed_data,k)\n",
    "    return count_emissions_df/count_y #e(x|y), where x is the tweet, and y is the sentiment\n",
    "\n",
    "#calculate_new_emission_counts(obtain_data('sg_train'),3)\n",
    "new_em_df_parameters = get_new_emission_params(obtain_data('sg_train'),1)\n",
    "#new_em_df_parameters.sum(axis=1) #gives the sum of each rows (individual respective words)\n",
    "'''eg.\n",
    "Throwing          0.000012\n",
    "headquarters      0.000300\n",
    "insist            0.000012\n",
    "except            0.000071\n",
    "Broken            0.000128\n",
    "LCD               0.000124\n",
    "occur             0.000012\n",
    "sound             0.000071\n",
    "'''\n",
    "\n",
    "new_em_df_parameters.sum(axis=0) #gives the counts of the sentiments\n",
    "new_em_df_parameters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_dataset(file):\n",
    "    \"\"\"\n",
    "    :param file: input preprocessed dataset \n",
    "    :return: new emissions params given preprocessed data and k\n",
    "    \"\"\"\n",
    "    dataset = obtain_data(file)\n",
    "    k = 1\n",
    "    return get_new_emission_params(dataset,k)\n",
    "\n",
    "#single sentiment analysis for a word\n",
    "def sentiment_analysis(emission_param,x):\n",
    "    \"\"\"\n",
    "    :param emission_param: new emissions params dataframe,\n",
    "    :param x: word that you want to predict the sentiment for \n",
    "    :return: predicted sentiment\n",
    "    \"\"\"\n",
    "    #checking if the tweet is an undiscovered/discovered word\n",
    "    #if the word does not appear in training set, then change it to #UNK#\n",
    "    #print(emission_param.index) #gives the individual tweets\n",
    "    if x not in emission_param.index:\n",
    "        x = '#UNK#'\n",
    "    probability = emission_param.loc[x,:]\n",
    "    max_probability = None\n",
    "    for col in probability.index:\n",
    "        #print(col) #gives the sentiments labels\n",
    "        '''B-positive\n",
    "            ..\n",
    "            ...\n",
    "            -\n",
    "            I-negative\n",
    "            B-neutral\n",
    "            242\n",
    "            O\n",
    "            477\n",
    "            B-negative\n",
    "            .\n",
    "            I-positive\n",
    "            I-neutral'''\n",
    "        if max_probability is None : \n",
    "            max_probability = probability.loc[col]\n",
    "            y = col\n",
    "        elif probability.loc[col]>max_probability: #take the max prob\n",
    "            max_probability = probability.loc[col]\n",
    "            y = col #take the sentiment with the highest probability\n",
    "        \n",
    "    return y\n",
    "        \n",
    "\n",
    "def evaluation(filename,emission_param,outputfile):\n",
    "    \"\"\"\n",
    "    :param filename: input datafile\n",
    "    :param emission_param: emission param dataframe \n",
    "    \"\"\"\n",
    "    with open(filename,'r',encoding=\"utf8\") as inputfile:\n",
    "        lines = inputfile.readlines()\n",
    "        lines = [line.replace('\\n','') for line in lines]\n",
    "        #print(lines)\n",
    "        '''['best', 'friends', 'who', 'cry', 'on', 'FaceTime', 'together', ',', 'stay', 'together', '', \"I'm\", 'at', 'Starbucks',\n",
    "        'in', 'Johor', 'Bahru', ',', 'Johor', 'w', '/', '@cassiecr17', 'https://t.co/3rzoTtjRag', '', 'Reports', 'of', 'a', \n",
    "        'collision', 'on', 'Friary', 'Road', 'in', 'Naas', 'https://t.co/MZgfLNdbyr', '', '♫', 'She', 'Moves', 'In', 'Her', 'Own' ......]\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i] #each individual tweets\n",
    "            if line != '': #if line is not empty\n",
    "                line = line + ' ' + sentiment_analysis(emission_param,line)\n",
    "            line += '\\n'\n",
    "            lines[i] = line\n",
    "            \n",
    "        with open(outputfile,\"w\",encoding=\"utf8\") as outputfile:\n",
    "            for line in lines:\n",
    "                outputfile.write(line)\n",
    "    print(\"evaluation completed!\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-positive\n",
      "B-negative    0.000000\n",
      "B-positive    0.000000\n",
      "O             0.000000\n",
      "I-positive    0.000235\n",
      "I-negative    0.000000\n",
      "I-neutral     0.000000\n",
      "B-neutral     0.000000\n",
      "Name: #UNK#, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I-positive'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_em_params = get_new_emission_params(obtain_data('SG\\train'),1)\n",
    "word = 'Experimental'\n",
    "try:\n",
    "    print(modified_em_params.loc[word]) #displays the probability of the sentiments tagged\n",
    "except:\n",
    "    word = '#UNK#' #if the tweet is replaced with #UNK#\n",
    "    print(modified_em_params.loc[word]) #displays the probability of the sentiments tagged\n",
    "sentiment_analysis(modified_em_params,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing it for all 4 countries\n",
    "for cty in [\"CN\",\"EN\",\"FR\",\"SG\"]:\n",
    "    emission_params = training_dataset(cty+\"/train\") #new emission params are also obtained inside here\n",
    "    print(\"Analysing for \" + cty + \" dataset:\")\n",
    "    evaluation(cty+\"/dev.in\",emission_params,cty+\"/dev.p2.out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >python3 evalResult.py FR/dev.out FR/dev.p2.out\n",
    "Entity in gold data: 238\n",
    "Entity in prediction: 1111\n",
    "\n",
    "Correct Entity : 180\n",
    "Entity  precision: 0.1620\n",
    "Entity  recall: 0.7563\n",
    "Entity  F: 0.2669\n",
    "\n",
    "Correct Entity Type : 75\n",
    "Entity Type  precision: 0.0675\n",
    "Entity Type  recall: 0.3151\n",
    "Entity Type  F: 0.1112\n",
    "\n",
    "##### >python3 evalResult.py EN/dev.out EN/dev.p2.out\n",
    "Entity in gold data: 802\n",
    "Entity in prediction: 1126\n",
    "\n",
    "Correct Entity : 624\n",
    "Entity  precision: 0.5542\n",
    "Entity  recall: 0.7781\n",
    "Entity  F: 0.6473\n",
    "\n",
    "Correct Entity Type : 508\n",
    "Entity Type  precision: 0.4512\n",
    "Entity Type  recall: 0.6334\n",
    "Entity Type  F: 0.5270\n",
    "\n",
    "#### >python3 evalResult.py CN/dev.out CN/dev.p2.out\n",
    "Entity in gold data: 1081\n",
    "Entity in prediction: 5001\n",
    "\n",
    "Correct Entity : 595\n",
    "Entity  precision: 0.1190\n",
    "Entity  recall: 0.5504\n",
    "Entity  F: 0.1957\n",
    "\n",
    "Correct Entity Type : 373\n",
    "Entity Type  precision: 0.0746\n",
    "Entity Type  recall: 0.3451\n",
    "Entity Type  F: 0.1227\n",
    "\n",
    "#### >python3 evalResult.py SG/dev.out SG/dev.p2.out\n",
    "Entity in gold data: 4092\n",
    "Entity in prediction: 12062\n",
    "\n",
    "Correct Entity : 2398\n",
    "Entity  precision: 0.1988\n",
    "Entity  recall: 0.5860\n",
    "Entity  F: 0.2969\n",
    "\n",
    "Correct Entity Type : 1295\n",
    "Entity Type  precision: 0.1074\n",
    "Entity Type  recall: 0.3165\n",
    "Entity Type  F: 0.1603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
