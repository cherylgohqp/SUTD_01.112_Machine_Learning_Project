{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(file):\n",
    "    data = []\n",
    "    dictionary = {}\n",
    "    allKeys = []\n",
    "    allVals = []\n",
    "    \n",
    "    with open(file,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        #print(len(lines))\n",
    "        index=0\n",
    "        for i in range (len(lines)-2000):\n",
    "            #print(lines[i]) #eg. 'Omg O' is line[0]\n",
    "            if lines[i] == '\\n':\n",
    "                data.append(lines[index:i]) #append everything until it encounters a \\n\n",
    "                index = i+1\n",
    "            lines[i] = lines[i].replace('\\n','')# replace the \\n at the end of each sentiments\n",
    "            lines[i] = lines[i].split(' ') #split the line into their respective parts  \n",
    "            #print(lines[i]) ##format = eg. ['Justin', 'B-neutral']\n",
    "        #convert to keys and values (dict)\n",
    "        for i in range(len(data)):\n",
    "            #print(data[i])\n",
    "            data_values = data[i]\n",
    "            key = [word[0] for word in data_values] #tweet\n",
    "            val = [word[1] for word in data_values] #sentiment\n",
    "            #print(key)\n",
    "            #print(val)\n",
    "            data[i] = [key,val]\n",
    "            #dictionary = dict(zip(key,val))\n",
    "            #print(data[i])\n",
    "           # print(dictionary)\n",
    "        #data[i][0] gives the tweets\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i][0])):\n",
    "                allVals.append(data[i][1][j]) #appending the sentiments corresponding to the tweets\n",
    "             \n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i][0])):\n",
    "                #print(data[i][0][j]) #give each word\n",
    "                allKeys.append(data[i][0][j])\n",
    "        setKeys = set(allKeys)\n",
    "        setVals = set(allVals)   \n",
    "\n",
    "    return dict(data=data,x_set=setKeys,y_set=setVals)\n",
    "\n",
    "#obtain_data('SG\\train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: (5 pts) Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>242</th>\n",
       "      <th>B-neutral</th>\n",
       "      <th>-</th>\n",
       "      <th>I-negative</th>\n",
       "      <th>B-positive</th>\n",
       "      <th>.</th>\n",
       "      <th>I-positive</th>\n",
       "      <th>I-neutral</th>\n",
       "      <th>O</th>\n",
       "      <th>B-negative</th>\n",
       "      <th>477</th>\n",
       "      <th>...</th>\n",
       "      <th>..</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#mufc</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persecution</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hode</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Schaumburg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  242  B-neutral    -  I-negative  B-positive    .  \\\n",
       "#mufc        0.0  0.0   0.000056  0.0         0.0    0.000000  0.0   \n",
       "persecution  0.0  0.0   0.000000  0.0         0.0    0.000000  0.0   \n",
       "hode         0.0  0.0   0.000000  0.0         0.0    0.000154  0.0   \n",
       "Max          0.0  0.0   0.000168  0.0         0.0    0.000308  0.0   \n",
       "#Schaumburg  0.0  0.0   0.000056  0.0         0.0    0.000000  0.0   \n",
       "\n",
       "             I-positive  I-neutral         O  B-negative  477  ...   ..  \n",
       "#mufc               0.0        0.0  0.000008    0.000000  0.0  0.0  0.0  \n",
       "persecution         0.0        0.0  0.000008    0.000000  0.0  0.0  0.0  \n",
       "hode                0.0        0.0  0.000000    0.000000  0.0  0.0  0.0  \n",
       "Max                 0.0        0.0  0.000000    0.000292  0.0  0.0  0.0  \n",
       "#Schaumburg         0.0        0.0  0.000000    0.000000  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#e(x|y) = Count(y -> x)/Count(y)\n",
    "#Count(y->x) means number of times you see x generated from y\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_emission_count(parsed_data):\n",
    "    data = parsed_data['data']\n",
    "    x_set = parsed_data['x_set']\n",
    "    y_set = parsed_data['y_set']\n",
    "    #create a new datafram of zeros with keys (ie.tweets) as the index and sentiments as the columns\n",
    "    count_emissions_df = pd.DataFrame(np.zeros((len(x_set),len(y_set))),index=x_set,columns=y_set)\n",
    "    count_y = pd.Series(np.zeros(len(y_set)),index=y_set) #create a series object of zeros with index as the sentiments => to store the number times the sentiments appear\n",
    "    #print(count_y)\n",
    "    #print(count_emissions_df) #datafram structure: where its tweets against columns of sentiments\n",
    "    \n",
    "    for word in data:\n",
    "        #print(word) #format of data => [[keys],[values]]\n",
    "        #keys are the tweets, values are the sentiments\n",
    "        tweets_data,sentiments_data = word\n",
    "        \n",
    "        for i in range(len(tweets_data)):\n",
    "            tweet,sentiment = tweets_data[i],sentiments_data[i] #associate the tweet with its sentiment\n",
    "            #print(tweet,sentiment)\n",
    "            #print(sentiment)\n",
    "            #+1 to the row,col, given the tweet, sentiment freq +1\n",
    "            count_emissions_df.loc[tweet,sentiment] += 1 #.loc[] access a grp of rows and columns by labels\n",
    "            #count_emissions_df is for Count(y->x) [counting the number of times a sentiment wrt to the tweet]\n",
    "            count_y[sentiment] += 1 #incrementing the number of time the respective sentiment appear\n",
    "    return count_emissions_df,count_y\n",
    "    \n",
    "def get_emission_params(parsed_data):\n",
    "    count_emissions_df,count_y = calculate_emission_count(parsed_data)\n",
    "    return count_emissions_df/count_y #e(x|y), where x is the tweet, and y is the sentiment\n",
    "\n",
    "\n",
    "em_df = get_emission_params(obtain_data('SG\\train'))\n",
    "#get_emission_counts(obtain_data('sg_train'))\n",
    "em_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Omg', \"I'm\", 'craving', 'UK', 'food', 'so', 'much', 'right', 'now', 'and', 'all', 'I', 'wanna', 'do', 'is', 'go', 'to', 'tescos', 'Mini', 'cheddars', ',', \"Terry's\", 'chocolate', 'orange', 'and', 'a', 'jam', 'donut', 'PLZZZZZ']\n",
      "['O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'I-positive', 'I-positive', 'O', 'B-positive', 'I-positive', 'I-positive', 'O', 'O', 'B-positive', 'I-positive', 'O']\n"
     ]
    }
   ],
   "source": [
    "#for testing purposes\n",
    "a=[['Omg', \"I'm\", 'craving', 'UK', 'food', 'so', 'much', 'right', 'now', 'and', 'all', 'I', 'wanna', 'do', 'is', 'go', 'to', 'tescos', 'Mini', 'cheddars', ',', \"Terry's\", 'chocolate', 'orange', 'and', 'a', 'jam', 'donut', 'PLZZZZZ'], ['O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'I-positive', 'I-positive', 'O', 'B-positive', 'I-positive', 'I-positive', 'O', 'O', 'B-positive', 'I-positive', 'O']]\n",
    "\n",
    "x,y = a\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b) (10 pts) One problem with estimating the emission parameters is that some words that appear in the test set do not appear in the training set. One simple idea to handle this issue is as follows. First, replace those words that appear less than k times in the training set with a special token #UNK# before training. This leads to a “modiﬁed training set”. We then use such a modiﬁed training set to train our model. During the testing phase, if the word does not appear in the “modiﬁed training set”, we replace that word with #UNK# as well. Set k to 1, implement this ﬁx into your function for computing the emission parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              1.0\n",
       "242           1.0\n",
       "B-neutral     1.0\n",
       "-             1.0\n",
       "I-negative    1.0\n",
       "B-positive    1.0\n",
       ".             1.0\n",
       "I-positive    1.0\n",
       "I-neutral     1.0\n",
       "O             1.0\n",
       "B-negative    1.0\n",
       "477           1.0\n",
       "...           1.0\n",
       "..            1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_new_emission_counts(parsed_data,k):\n",
    "    count_emissions_df,count_y = calculate_emission_count(parsed_data)\n",
    "    #.sum(axis = 1(sum the column), axis = 0 (sum the index))\n",
    "    count_tweet_appearance = count_emissions_df.sum(axis=1)\n",
    "    #print(count_tweet_appearance) #counting the number of times each tweet appears by summing everything across the columns\n",
    "    '''Output of count_tweet appearance eg. \n",
    "        seems                      15.0\n",
    "        https://t.co/h6Ie4IBJ08     1.0\n",
    "        #AnnaVonHausswolff          2.0\n",
    "        Bowery                      2.0\n",
    "        refuge                      2.0\n",
    "        @chuckielufc                1.0\n",
    "        https://t.co/7xSNeWemp1     1.0\n",
    "        @chris_steller              3.0\n",
    "        unexpected                  3.0\n",
    "        #usantdp                    1.0\n",
    "        Ones                        2.0\n",
    "        1979                        4.0\n",
    "        @joceltsh                   1.0\n",
    "        @TomBoxingAsylum            2.0\n",
    "        @thistletat13               2.0\n",
    "        @eibeibb                    2.0\n",
    "        @TalatHussain12             1.0\n",
    "        Ilkeston                    2.0\n",
    "        @ricosua                    1.0\n",
    "        Belarus                     2.0\n",
    "        charms                      2.0\n",
    "        @EvermorSolution            2.0\n",
    "        https://t.co/WrcuWKQ0Xg     2.0\n",
    "        FIRED                       2.0'''\n",
    "    \n",
    "    failed_tweets = count_tweet_appearance[count_tweet_appearance<k]\n",
    "    #print(failed_tweets)\n",
    "    '''eg output if k<3 (ie. tweets with occurence less than 3 times) is:\n",
    "        @Nandos                    1.0\n",
    "        @rcmpgrcpolice             1.0\n",
    "        #yas                       1.0\n",
    "        @just                      2.0\n",
    "        ford                       1.0\n",
    "        attracted                  2.0\n",
    "        @Unitetheunion             1.0\n",
    "        .....        '''\n",
    "    \n",
    "    #replace the tweets that occur less than 1.0 with \"#UNK#\"\n",
    "    #print(failed_tweets.index) #gives all the tweets that <1.0\n",
    "    \n",
    "    replace_tweets = count_emissions_df.loc[failed_tweets.index].sum(axis=0)\n",
    "    replace_tweets.name  = '#UNK#'\n",
    "    \n",
    "    new_df = count_emissions_df.append(replace_tweets)\n",
    "    new_df = new_df.drop(failed_tweets.index,axis=0) #drop all failed_tweets words\n",
    "    headers = new_df.dtypes.index\n",
    "    #print(headers[4]) #gives sentiment O\n",
    "    new_df.at['#UNK#',headers[4]] = 1.0\n",
    "    #print(new_df) #without failed_tweets words inside, has #UNK# row inside at the bottom\n",
    "    \n",
    "    return new_df, count_y\n",
    "\n",
    "def get_new_emission_params(parsed_data,k):\n",
    "    count_emissions_df,count_y = calculate_new_emission_counts(parsed_data,k)\n",
    "    return count_emissions_df/count_y #e(x|y), where x is the tweet, and y is the sentiment\n",
    "\n",
    "#calculate_new_emission_counts(obtain_data('sg_train'),3)\n",
    "new_em_df_parameters = get_new_emission_params(obtain_data('sg_train'),1)\n",
    "#new_em_df_parameters.sum(axis=1) #gives the sum of each rows (individual respective words)\n",
    "'''eg.\n",
    "Throwing          0.000012\n",
    "headquarters      0.000300\n",
    "insist            0.000012\n",
    "except            0.000071\n",
    "Broken            0.000128\n",
    "LCD               0.000124\n",
    "occur             0.000012\n",
    "sound             0.000071\n",
    "'''\n",
    "\n",
    "new_em_df_parameters.sum(axis=0) #gives the counts of the sentiments\n",
    "#new_em_df_parameters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/dcoE3.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2c Implement a simple sentiment analysis system that produces the tag => y∗ = argmax e(x|y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_dataset(file):\n",
    "    dataset = obtain_data(file)\n",
    "    k = 1\n",
    "    return get_new_emission_params(dataset,k)\n",
    "\n",
    "#single sentiment analysis for a word\n",
    "def sentiment_analysis(emission_param,x):\n",
    "    #checking if the tweet is an undiscovered/discovered word\n",
    "    #if the word does not appear in training set, then change it to #UNK#\n",
    "    #print(emission_param.index) #gives the individual tweets\n",
    "    if x not in emission_param.index:\n",
    "        x = '#UNK#'\n",
    "    probability = emission_param.loc[x,:]\n",
    "   # print(probability)\n",
    "    max_probability = None\n",
    "    for col in probability.index:\n",
    "        #print(col) #gives the sentiments labels\n",
    "        '''B-positive\n",
    "            ..\n",
    "            ...\n",
    "            -\n",
    "            I-negative\n",
    "            B-neutral\n",
    "            242\n",
    "            O\n",
    "            477\n",
    "            B-negative\n",
    "            .\n",
    "            I-positive\n",
    "            I-neutral'''\n",
    "        if col == 'O':\n",
    "            probability.loc[col] += 0.0001\n",
    "            #print(\"Im here\")\n",
    "            #y = col\n",
    "        if max_probability is None : \n",
    "            max_probability = probability.loc[col]\n",
    "            y = col\n",
    "        elif probability.loc[col]>max_probability: #take the max prob\n",
    "            max_probability = probability.loc[col]\n",
    "            y = col #take the sentiment with the highest probability\n",
    "        \n",
    "    return y\n",
    "        \n",
    "\n",
    "def evaluation(filename,emission_param,outputfile):\n",
    "    with open(filename,'r',encoding=\"utf8\") as inputfile:\n",
    "        lines = inputfile.readlines()\n",
    "        lines = [line.replace('\\n','') for line in lines]\n",
    "        #print(lines)\n",
    "        '''['best', 'friends', 'who', 'cry', 'on', 'FaceTime', 'together', ',', 'stay', 'together', '', \"I'm\", 'at', 'Starbucks',\n",
    "        'in', 'Johor', 'Bahru', ',', 'Johor', 'w', '/', '@cassiecr17', 'https://t.co/3rzoTtjRag', '', 'Reports', 'of', 'a', \n",
    "        'collision', 'on', 'Friary', 'Road', 'in', 'Naas', 'https://t.co/MZgfLNdbyr', '', '♫', 'She', 'Moves', 'In', 'Her', 'Own' ......]\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i] #each individual tweets\n",
    "            if line != '': #if line is not empty\n",
    "                line = line + ' ' + sentiment_analysis(emission_param,line)\n",
    "            line += '\\n'\n",
    "            lines[i] = line\n",
    "            \n",
    "        with open(outputfile,\"w\",encoding=\"utf8\") as outputfile:\n",
    "            for line in lines:\n",
    "                outputfile.write(line)\n",
    "    print(\"evaluation completed!\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1.000000\n",
      "242           0.000000\n",
      "B-neutral     0.000000\n",
      "-             0.000000\n",
      "I-negative    0.021164\n",
      "B-positive    0.000000\n",
      ".             0.900000\n",
      "I-positive    0.015486\n",
      "I-neutral     0.009963\n",
      "O             0.030366\n",
      "B-negative    0.000000\n",
      "477           0.000000\n",
      "...           0.833333\n",
      "..            1.000000\n",
      "Name: ., dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_em_params = get_new_emission_params(obtain_data('sg_train'),1)\n",
    "word = '.'\n",
    "try:\n",
    "    print(modified_em_params.loc[word])\n",
    "except:\n",
    "    word = '#UNK#'\n",
    "    print(modified_em_params.loc[word])\n",
    "sentiment_analysis(modified_em_params,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "emission_param = training_dataset('SG/train')\n",
    "evaluation('SG/dev.in',emission_param,'SG/dev.p2.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing for CN dataset:\n",
      "evaluation completed!\n",
      "Analysing for EN dataset:\n",
      "evaluation completed!\n",
      "Analysing for FR dataset:\n",
      "evaluation completed!\n",
      "Analysing for SG dataset:\n",
      "evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "#doing it for all 4 countries\n",
    "for cty in [\"CN\",\"EN\",\"FR\",\"SG\"]:\n",
    "    emission_params = training_dataset(cty+\"/train\") #new emission params are also obtained inside here\n",
    "    print(\"Analysing for \" + cty + \" dataset:\")\n",
    "    evaluation(cty+\"/dev.in\",emission_params,cty+\"/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 4092\n",
      "#Entity in prediction: 6024\n",
      "\n",
      "#Correct Entity : 1545\n",
      "Entity  precision: 0.2565\n",
      "Entity  recall: 0.3776\n",
      "Entity  F: 0.3055\n",
      "\n",
      "#Correct Entity Type : 899\n",
      "Entity Type  precision: 0.1492\n",
      "Entity Type  recall: 0.2197\n",
      "Entity Type  F: 0.1777\n"
     ]
    }
   ],
   "source": [
    "###adapted from evalResult.py\n",
    "import sys\n",
    "import re\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "#Read entities from predcition\n",
    "def get_predicted(predicted, answers=defaultdict(lambda: defaultdict(defaultdict))):\n",
    "\n",
    "    example = 0\n",
    "    word_index = 0\n",
    "    entity = []\n",
    "    last_ne = \"O\"\n",
    "    last_sent = \"\"\n",
    "    last_entity = []\n",
    "\n",
    "    answers[example] = []\n",
    "    for line in predicted:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"##\"):\n",
    "            continue\n",
    "        if len(line)==1 and line=='.':\n",
    "            continue\n",
    "        elif len(line) == 0:\n",
    "            if entity:\n",
    "                answers[example].append(list(entity))\n",
    "                entity = []\n",
    "\n",
    "            example += 1\n",
    "            answers[example] = []\n",
    "            word_index = 0\n",
    "            last_ne = \"O\"\n",
    "            continue\n",
    "        else:\n",
    "            split_line = line.split(separator)\n",
    "            #word = split_line[0]\n",
    "            value = split_line[outputColumnIndex]\n",
    "            ne = value[0]\n",
    "            sent = value[2:]\n",
    "\n",
    "\n",
    "            last_entity = []\n",
    "\n",
    "            #check if it is start of entity\n",
    "            if ne == 'B' or (ne == 'I' and last_ne == 'O') or (last_ne != 'O' and ne == 'I' and last_sent != sent):\n",
    "                if entity:\n",
    "                    last_entity = list(entity)\n",
    "\n",
    "                entity = [sent]\n",
    "                    \n",
    "                entity.append(word_index)\n",
    "\n",
    "            elif ne == 'I':\n",
    "                entity.append(word_index)\n",
    "\n",
    "            elif ne == 'O':\n",
    "                if last_ne == 'B' or last_ne == 'I':\n",
    "                    last_entity =list(entity)\n",
    "                entity = []\n",
    "\n",
    "\n",
    "            if last_entity:\n",
    "                answers[example].append(list(last_entity))\n",
    "                last_entity = []\n",
    "\n",
    "        last_sent = sent\n",
    "        last_ne = ne\n",
    "        word_index += 1\n",
    "\n",
    "    if entity:\n",
    "        answers[example].append(list(entity))\n",
    "\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "#Read entities from gold data\n",
    "def get_observed(observed):\n",
    "\n",
    "\n",
    "    example = 0\n",
    "    word_index = 0\n",
    "    entity = []\n",
    "    last_ne = \"O\"\n",
    "    last_sent = \"\"\n",
    "    last_entity = []\n",
    "\n",
    "    observations=defaultdict(defaultdict)\n",
    "    observations[example] = []\n",
    "\n",
    "    for line in observed:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"##\"):\n",
    "            continue\n",
    "        elif len(line) == 0:\n",
    "            if entity:\n",
    "                observations[example].append(list(entity))\n",
    "                entity = []\n",
    "\n",
    "            example += 1\n",
    "            observations[example] = []\n",
    "            word_index = 0\n",
    "            last_ne = \"O\"\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            split_line = line.split(separator)\n",
    "            word = split_line[0]\n",
    "            value = split_line[outputColumnIndex]\n",
    "            ne = value[0]\n",
    "            sent = value[2:]\n",
    "\n",
    "\n",
    "            last_entity = []\n",
    "\n",
    "            #check if it is start of entity, suppose there is no weird case in gold data\n",
    "            if ne == 'B' or (ne == 'I' and last_ne == 'O') or (last_ne != 'O' and ne == 'I' and last_sent != sent):\n",
    "                if entity:\n",
    "                    last_entity = entity\n",
    "\n",
    "                entity = [sent]\n",
    "                    \n",
    "                entity.append(word_index)\n",
    "\n",
    "            elif ne == 'I':\n",
    "                entity.append(word_index)\n",
    "\n",
    "            elif ne == 'O':\n",
    "                if last_ne == 'B' or last_ne == 'I':\n",
    "                    last_entity = entity\n",
    "                entity = []\n",
    "\n",
    "\n",
    "            if last_entity:\n",
    "                observations[example].append(list(last_entity))\n",
    "                last_entity = []\n",
    "\n",
    "\n",
    "        last_ne = ne\n",
    "        last_sent = sent\n",
    "        word_index += 1\n",
    "\n",
    "    if entity:\n",
    "        observations[example].append(list(entity))\n",
    "\n",
    "    return observations\n",
    "\n",
    "#Print Results and deal with division by 0\n",
    "def printResult(evalTarget, num_correct, prec, rec):\n",
    "    if abs(prec + rec ) < 1e-6:\n",
    "        f = 0\n",
    "    else:\n",
    "        f = 2 * prec * rec / (prec + rec)\n",
    "    print('#Correct', evalTarget, ':', num_correct)\n",
    "    print(evalTarget, ' precision: %.4f' % (prec))\n",
    "    print(evalTarget, ' recall: %.4f' %   (rec))\n",
    "    print(evalTarget, ' F: %.4f' % (f))\n",
    "\n",
    "#Compare results bewteen gold data and prediction data\n",
    "def compare_observed_to_predicted(observed, predicted):\n",
    "\n",
    "    correct_sentiment = 0\n",
    "    correct_entity = 0\n",
    "\n",
    "    total_observed = 0.0\n",
    "    total_predicted = 0.0\n",
    "\n",
    "    #For each Instance Index example (example = 0,1,2,3.....)\n",
    "    for example in observed:\n",
    "\n",
    "        if example in discardInstance:\n",
    "            continue\n",
    "\n",
    "        observed_instance = observed[example]\n",
    "        predicted_instance = predicted[example]\n",
    "\n",
    "        #Count number of entities in gold data\n",
    "        total_observed += len(observed_instance)\n",
    "        #Count number of entities in prediction data\n",
    "        total_predicted += len(predicted_instance)\n",
    "\n",
    "        #For each entity in prediction\n",
    "        for span in predicted_instance:\n",
    "            span_begin = span[1]\n",
    "            span_length = len(span) - 1\n",
    "            span_ne = (span_begin, span_length)\n",
    "            span_sent = span[0]\n",
    "\n",
    "            #For each entity in gold data\n",
    "            for observed_span in observed_instance:\n",
    "                begin = observed_span[1]\n",
    "                length = len(observed_span) - 1\n",
    "                ne = (begin, length)\n",
    "                sent = observed_span[0]\n",
    "\n",
    "                #Entity matched\n",
    "                if span_ne == ne:\n",
    "                    correct_entity += 1\n",
    "\n",
    "\n",
    "                    #Entity & Sentiment both are matched\n",
    "                    if span_sent == sent:\n",
    "                        correct_sentiment += 1\n",
    "\n",
    "    print()\n",
    "    print('#Entity in gold data: %d' % (total_observed))\n",
    "    print('#Entity in prediction: %d' % (total_predicted))\n",
    "    print()\n",
    "\n",
    "    prec = correct_entity/total_predicted\n",
    "    rec = correct_entity/total_observed\n",
    "    printResult('Entity', correct_entity, prec, rec)\n",
    "    print()\n",
    "\n",
    "    prec = correct_sentiment/total_predicted\n",
    "    rec = correct_sentiment/total_observed\n",
    "    printResult('Entity Type',correct_sentiment, prec, rec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############Main Function##################\n",
    "\n",
    "if len(sys.argv) < 3:\n",
    "    print ('Please make sure you have installed Python 3.4 or above!')\n",
    "    print (\"Usage on Windows:  python evalResult.py [gold file] [prediction file]\")\n",
    "    print (\"Usage on Linux/Mac:  python3 evalResult.py [gold file] [prediction file]\")\n",
    "    sys.exit()\n",
    "\n",
    "gold = open('SG\\dev.out', \"r\", encoding='UTF-8')\n",
    "prediction = open('SG\\dev.p2.out', \"r\", encoding='UTF-8')\n",
    "discardInstance = []\n",
    "\n",
    "\n",
    "if len(sys.argv) > 3 and sys.argv[3] == 'filter':\n",
    "    filterInst_file = open(sys.argv[1] + '.filter', \"r\", encoding='UTF-8')\n",
    "    for line in filterInst_file:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\r')\n",
    "        instID = int(line)\n",
    "        discardInstance.append(instID)\n",
    "\n",
    "\n",
    "#column separator\n",
    "separator = ' '\n",
    "\n",
    "#the column index for tags\n",
    "outputColumnIndex = 1\n",
    "#Read Gold data\n",
    "observed = get_observed(gold)\n",
    "\n",
    "#Read Predction data\n",
    "predicted = get_predicted(prediction)\n",
    "\n",
    "#Compare\n",
    "compare_observed_to_predicted(observed, predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >python3 evalResult.py EN/dev.out EN/dev.p2.out\n",
    "\n",
    "Entity in gold data: 802\n",
    "Entity in prediction: 754\n",
    "\n",
    "Correct Entity : 481\n",
    "Entity  precision: 0.6379\n",
    "Entity  recall: 0.5998\n",
    "Entity  F: 0.6183\n",
    "\n",
    "Correct Entity Type : 441\n",
    "Entity Type  precision: 0.5849\n",
    "Entity Type  recall: 0.5499\n",
    "Entity Type  F: 0.5668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >python3 evalResult.py CN/dev.out CN/dev.p2.out\n",
    "Entity in gold data: 1081\n",
    "Entity in prediction: 2990\n",
    "\n",
    "Correct Entity : 463\n",
    "Entity  precision: 0.1548\n",
    "Entity  recall: 0.4283\n",
    "Entity  F: 0.2275\n",
    "\n",
    "Correct Entity Type : 330\n",
    "Entity Type  precision: 0.1104\n",
    "Entity Type  recall: 0.3053\n",
    "Entity Type  F: 0.1621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >python3 evalResult.py FR/dev.out FR/dev.p2.out\n",
    "Entity in gold data: 238\n",
    "Entity in prediction: 749\n",
    "\n",
    "Correct Entity : 176\n",
    "Entity  precision: 0.2350\n",
    "Entity  recall: 0.7395\n",
    "Entity  F: 0.3566\n",
    "\n",
    "Correct Entity Type : 77\n",
    "Entity Type  precision: 0.1028\n",
    "Entity Type  recall: 0.3235\n",
    "Entity Type  F: 0.1560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >python3 evalResult.py SG/dev.out SG/dev.p2.out\n",
    "Entity in gold data: 4092\n",
    "Entity in prediction: 6024\n",
    "\n",
    "Correct Entity : 1545\n",
    "Entity  precision: 0.2565\n",
    "Entity  recall: 0.3776\n",
    "Entity  F: 0.3055\n",
    "\n",
    "Correct Entity Type : 899\n",
    "Entity Type  precision: 0.1492\n",
    "Entity Type  recall: 0.2197\n",
    "Entity Type  F: 0.1777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
